{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import concurrent.futures\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_match_hists = pd.read_csv('../data/challenger_match_hists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_base_url_dict = {'na':'na1.api.riotgames.com',\n",
    "          'br':'br1.api.riotgames.com',\n",
    "          'eun':'eun1.api.riotgames.com',\n",
    "          'euw':'euw1.api.riotgames.com',\n",
    "          'jp':'jp1.api.riotgames.com',\n",
    "          'kr':'kr.api.riotgames.com',\n",
    "          'la1':'la1.api.riotgames.com',\n",
    "          'la2':'la2.api.riotgames.com',\n",
    "          'oce':'oc1.api.riotgames.com',\n",
    "          'tr':'tr1.api.riotgames.com',\n",
    "          'ru':'ru.api.riotgames.com'}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = pd.read_json('../secrets.json')['riot_api_key'][0]\n",
    "\n",
    "timeline_by_match_id_url = '/lol/match/v4/timelines/by-match/'\n",
    "\n",
    "match_by_match_id_url = '/lol/match/v4/matches/'\n",
    "\n",
    "account_by_name_url = '/lol/summoner/v4/summoners/by-name/'\n",
    "\n",
    "match_hist_by_id_url = '/lol/match/v4/matchlists/by-account/'\n",
    "\n",
    "challenger_ladder_url = '/lol/league/v4/challengerleagues/by-queue/RANKED_SOLO_5x5'\n",
    "\n",
    "summoner_by_summoner_id = '/lol/summoner/v4/summoners/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champions = requests.get('http://ddragon.leagueoflegends.com/cdn/10.16.1/data/en_US/champion.json').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champions_df = pd.DataFrame(champions['data']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#challenger_ladder = requests.get(f'https://na1.api.riotgames.com{challenger_ladder_url}?api_key={api_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#challenger_ladder.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_all_challengers():\n",
    "    challengers_df = pd.DataFrame()\n",
    "    for key, base_url in region_base_url_dict.items():\n",
    "        ladder_response = requests.get(f'https://{base_url}{challenger_ladder_url}?api_key={api_key}')\n",
    "        try:\n",
    "            assert(ladder_response.status_code == 200)\n",
    "            response_df = pd.DataFrame(ladder_response.json()['entries'])\n",
    "            response_df['region'] = [key] * len(response_df)\n",
    "            challengers_df = pd.concat([challengers_df,response_df])\n",
    "        except:\n",
    "            print(f'Bad request for {key}: {ladder_response.status_code}')\n",
    "        time.sleep(1.2001)\n",
    "        \n",
    "    return challengers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_summoner(base_url, summoner_id):\n",
    "    return requests.get(f'https://{base_url}{summoner_by_summoner_id}{summoner_id}?api_key={api_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_match_hist(account_id, base_url, queue = '420'):\n",
    "    return requests.get(f'https://{base_url}{match_hist_by_id_url}{account_id}?api_key={api_key}&queue={queue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(match_id, base_url):\n",
    "    return requests.get(f'https://{base_url}{match_by_match_id_url}{match_id}?api_key={api_key}'),requests.get(f'https://{base_url}{timeline_by_match_id_url}{match_id}?api_key={api_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_challenger_match_hists(region, challengers_df = all_challengers, has_account_id=False):\n",
    "    challengers_df = challengers_df[['summonerId','region']]\n",
    "    challengers_df['player_ids'] = [np.nan] * len(challengers_df)\n",
    "    challengers_df = challengers_df[challengers_df['region'] == region]\n",
    "    challengers_df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    if has_account_id==False:\n",
    "        for i, tuples in enumerate(challengers_df.values):\n",
    "            if i % 500 == 0:\n",
    "                print(f'{i} of {len(challengers_df)} account ids')\n",
    "            summoner_id, region, unused_pid = tuples\n",
    "            summoner = get_summoner(region_base_url_dict[region],summoner_id).json()\n",
    "            account_id = summoner['accountId']\n",
    "            challengers_df.loc[i,'account_ids'] = account_id\n",
    "            all_challengers.loc[i,'account_ids'] = account_id\n",
    "            time.sleep(1.2)\n",
    "        \n",
    "    challenger_match_hists = pd.DataFrame()\n",
    "    for i, account_id in enumerate(challengers_df['account_ids']):\n",
    "        if i % 500 == 0:\n",
    "            print(f'{i} of {len(challengers_df)} match histories')\n",
    "        try:\n",
    "            match_hist = pd.DataFrame(get_match_hist(account_id, region_base_url_dict[challengers_df.loc[i,'region']]).json()['matches'])\n",
    "            match_hist['region'] = [challengers_df.loc[i,'region']] * len(match_hist)\n",
    "            match_hist['account_id'] = [account_id] * len(match_hist)\n",
    "            challenger_match_hists = pd.concat([challenger_match_hists, match_hist])\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(1.2)\n",
    "    \n",
    "    return challenger_match_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def scrape_seeds(region):\n",
    "    print(f'{region} thread initialized')\n",
    "    start_time = time.time()\n",
    "    base_url = region_base_url_dict[region]\n",
    "    \n",
    "    matches_list = []\n",
    "    scraped_ids = set()\n",
    "    timelines_list = []\n",
    "    scraped_participants = set()\n",
    "    unscraped_ids = set()\n",
    "    scraped_matches = []\n",
    "    \n",
    "    with jsonlines.open(f'../data/scraped_ids_{region}.jsonl') as infile:\n",
    "        print(f'reading scraped_ids_{region}')\n",
    "        for line in infile.iter():\n",
    "            scraped_ids.add(line)       \n",
    "    \n",
    "    with jsonlines.open(f'../data/unscraped_ids_{region}.jsonl') as infile:\n",
    "            print(f'reading unscraped_ids_{region}')\n",
    "            for line in infile.iter():\n",
    "                unscraped_ids.add(line)\n",
    "                \n",
    "    with jsonlines.open(f'../data/matches_{region}.jsonl') as infile:\n",
    "            print(f'reading matches_{region}')\n",
    "            for line in infile.iter():\n",
    "                scraped_matches.append(dict(line)['gameId'])     \n",
    "    \n",
    "    seed = pd.read_csv('../data/challenger_match_hists_ranked_only.csv')\n",
    "    \n",
    "    seed = seed[seed['region']==region]\n",
    "    \n",
    "    seed.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    for i, account_id in enumerate(seed['account_id']):\n",
    "        if account_id not in scraped_ids or seed.loc[i,'gameId'] not in scraped_matches:\n",
    "            scraped_ids.add(account_id)\n",
    "            match, timeline = get_match(seed.loc[i,'gameId'], base_url)\n",
    "        \n",
    "            if match.status_code == 200 and timeline.status_code == 200:\n",
    "                matches_list.append(match.json())\n",
    "                timelines_list.append(timeline.json())\n",
    "                [scraped_participants.add(part['player']['accountId']) for part in match.json()['participantIdentities']]\n",
    "                scraped_matches.append(seed.loc[i,'gameId'])\n",
    "            else:\n",
    "                print(f'matches error: {match.status_code}\\ntimelines error: {timeline.status_code}\\nSummoner: {account_id}\\nRegion:{region}')\n",
    "            time.sleep(2.4)\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if 'match' in locals():\n",
    "            if i % 100 == 0 or match.status_code == 403:\n",
    "                print(f'{i} matches scraped of {len(seed)}\\nRegion: {region}\\n')\n",
    "                try:\n",
    "                    with open(f'../data/matches_{region}.jsonl', 'a') as outfile:\n",
    "                        for entry in matches_list:\n",
    "                            json.dump(entry, outfile)\n",
    "                            outfile.write('\\n')\n",
    "                            matches_list = []\n",
    "\n",
    "                    with open(f'../data/timelines_{region}.jsonl', 'a') as outfile:\n",
    "                        for entry in timelines_list:\n",
    "                            json.dump(entry, outfile)\n",
    "                            outfile.write('\\n')\n",
    "                            timelines_list = []\n",
    "\n",
    "                    with open(f'../data/scraped_ids_{region}.jsonl', 'w') as outfile:\n",
    "                        for entry in scraped_ids:\n",
    "                            json.dump(entry, outfile)\n",
    "                            outfile.write('\\n')\n",
    "\n",
    "                    unscraped_ids = unscraped_ids.union(scraped_participants)\n",
    "                    unscraped_ids -= scraped_ids\n",
    "\n",
    "                    with open(f'../data/unscraped_ids_{region}.jsonl', 'w') as outfile:\n",
    "                        for entry in unscraped_ids:\n",
    "                            json.dump(entry, outfile)\n",
    "                            outfile.write('\\n')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if match.status_code == 403:\n",
    "                    return None\n",
    "            \n",
    "    with open(f'../data/matches_{region}.jsonl', 'a') as outfile:\n",
    "        for entry in matches_list:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    with open(f'../data/timelines_{region}.jsonl', 'a') as outfile:\n",
    "        for entry in timelines_list:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    with open(f'../data/scraped_ids_{region}.jsonl', 'a') as outfile:\n",
    "        for entry in scraped_ids:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    print(f'Seeds scraped, region = {region}')\n",
    "\n",
    "    unscraped_ids = unscraped_ids.union(scraped_participants)\n",
    "    unscraped_ids -= scraped_ids\n",
    "    \n",
    "    with open(f'../data/unscraped_ids_{region}.jsonl', 'w') as outfile:\n",
    "        for entry in unscraped_ids:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(region):\n",
    "    match_list = []\n",
    "    timeline_list = []\n",
    "    \n",
    "    with jsonlines.open(f'../data/matches_{region}.jsonl') as infile:\n",
    "                for line in infile.iter():\n",
    "                    match_list.append(line) \n",
    "                    \n",
    "    with jsonlines.open(f'../data/timelines_{region}.jsonl') as infile:\n",
    "                for line in infile.iter():\n",
    "                    timeline_list.append(line) \n",
    "                    \n",
    "    ranked_matches = []\n",
    "    ranked_timelines = []\n",
    "    is_ranked = []\n",
    "    game_ids = set()\n",
    "    \n",
    "    for i, match in enumerate(match_list):\n",
    "        if match['queueId'] == 420 and match['gameId'] not in game_ids:\n",
    "            ranked_matches.append(match)\n",
    "            is_ranked.append(i)\n",
    "            game_ids.add(match['gameId'])\n",
    "\n",
    "    for i, timeline in enumerate(timeline_list):\n",
    "        if i in is_ranked:\n",
    "            ranked_timelines.append(timeline)\n",
    "            \n",
    "    with open(f'../data/cleaned/matches_{region}.jsonl', 'w') as outfile:\n",
    "        for entry in ranked_matches:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    with open(f'../data/cleaned/timelines_{region}.jsonl', 'w') as outfile:\n",
    "        for entry in ranked_timelines:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_games(region):\n",
    "    match_list = []\n",
    "    timeline_list = []\n",
    "    \n",
    "    with jsonlines.open(f'../data/cleaned/matches_{region}.jsonl') as infile:\n",
    "                for line in infile.iter():\n",
    "                    match_list.append(dict(line)['gameId']) \n",
    "                    \n",
    "    with jsonlines.open(f'../data/cleaned/timelines_{region}.jsonl') as infile:\n",
    "                for line in infile.iter():\n",
    "                    timeline_list.append(dict(line)['gameId']) \n",
    "                    \n",
    "    good_matches = []\n",
    "    good_timelines = []\n",
    "    valid = []\n",
    "    game_ids = []\n",
    "    \n",
    "    for i, match in enumerate(match_list):\n",
    "        if match['gameDuration'] / 60 > 15 and match['gameId'] not in game_ids:\n",
    "            add = True\n",
    "            # Remove games where a player didn't reach level 6 (based on EDA)\n",
    "            for participant in match['participants']:\n",
    "                if participant['stats']['champLevel'] < 7:\n",
    "                    add = False\n",
    "            if add:\n",
    "                good_matches.append(match)\n",
    "                valid.append(i)\n",
    "                game_ids.add(match['gameId'])\n",
    "\n",
    "    for i, timeline in enumerate(timeline_list):\n",
    "        if i in valid:\n",
    "            good_timelines.append(timeline)\n",
    "            \n",
    "    with open(f'../data/cleaned/matches_{region}.jsonl', 'w') as outfile:\n",
    "        for entry in good_matches:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    with open(f'../data/cleaned/timelines_{region}.jsonl', 'w') as outfile:\n",
    "        for entry in good_timelines:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import threading\n",
    "\n",
    "#  for region in region_base_url_dict.keys():\n",
    "#      some_thread = threading.Thread(target = scrape_seeds, args=[region])\n",
    "#      some_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_matches = []\n",
    "scraped_ids = set()\n",
    "\n",
    "with jsonlines.open(f'../data/scraped_ids_na.jsonl') as infile:\n",
    "        print(f'reading scraped_ids_na')\n",
    "        for line in infile.iter():\n",
    "            scraped_ids.add(line)\n",
    "\n",
    "with jsonlines.open(f'../data/matches_na.jsonl') as infile:\n",
    "            print(f'reading matches_na')\n",
    "            for line in infile.iter():\n",
    "                scraped_matches.append(dict(line)['gameId'])\n",
    "                \n",
    "na_hists = challenger_match_hists[challenger_match_hists['region'] == 'na']\n",
    "\n",
    "for i in range(1,len(na_hists)):\n",
    "    print(na_hists.loc[i,'gameId'] not in scraped_matches or na_hists.loc[i,'account_id'] not in scraped_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers = 11) as executor:\n",
    "    executor.map(scrape_seeds, region_base_url_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_challengers = get_all_challengers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_challengers.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spawwwwn = get_summoner(region_base_url_dict[all_challengers['region'][0]], all_challengers['summonerId'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spawwwwn.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_challengers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_challengers.loc[0,['summonerId','region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_match_hist(Spawwwwn.json()['accountId'],region_base_url_dict[all_challengers['region'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(test.json()['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#challenger_match_hists = get_challenger_match_hists(all_challengers)\n",
    "\n",
    "match_hist_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = 11) as executor:\n",
    "    for result in executor.map(get_challenger_match_hists, region_base_url_dict.keys()):\n",
    "        match_hist_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_match_hists = pd.concat(match_hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_match_hists.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_match_hists.to_csv('../data/challenger_match_hists_ranked_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_base_url_dict.keys():\n",
    "    remove_duplicates(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_base_url_dict.keys():\n",
    "    remove_short_games(region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
